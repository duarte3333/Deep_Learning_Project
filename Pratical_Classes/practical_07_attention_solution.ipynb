{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8c2e08a310>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(5)\n",
    "np.random.seed(5)\n",
    "torch.random.manual_seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([-2.0, 1.0, 0.5])\n",
    "x2 = np.array([1.0, 1.5, -0.5])\n",
    "x3 = np.array([-1.5, 1.0, -0.5])\n",
    "x4 = np.array([-2.0, -2.5, 1.5])\n",
    "\n",
    "X = np.array([x1, x2, x3, x4])\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[4.5 0.  4.5 0. ]\n",
      "[2.59807621 0.         2.59807621 0.        ]\n",
      "[0.46536883 0.03463117 0.46536883 0.03463117]\n",
      "[-1.66342208  0.8961065   0.03463117]\n"
     ]
    }
   ],
   "source": [
    "q = np.array([-2.0, 1.0, -1.0])\n",
    "\n",
    "print(np.size(q))\n",
    "\n",
    "scores = X.dot(q) / np.sqrt(np.size(q))\n",
    "probabilities = np.exp(scores) / np.sum(np.exp(scores))\n",
    "output = X.T.dot(probabilities)\n",
    "\n",
    "print(X.dot(q))\n",
    "print(scores)\n",
    "print(probabilities)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.25  4.5 ]\n",
      " [ 1.25  2.  ]\n",
      " [-1.25  4.75]\n",
      " [-2.75 -3.5 ]]\n",
      "[[ 5.75  1.5 ]\n",
      " [ 2.   -0.5 ]\n",
      " [ 4.5   2.  ]\n",
      " [-2.5   0.5 ]]\n",
      "[[-2.5  -7.5 ]\n",
      " [ 0.25  0.  ]\n",
      " [-2.   -5.25]\n",
      " [-0.75 -1.5 ]]\n",
      "1.4142135623730951\n",
      "[[ -4.37522321  -4.77297077  -0.79549513   5.5684659 ]\n",
      " [  7.20365033   1.06066017   6.80590277  -1.50260191]\n",
      " [ -0.04419417  -3.44714556   2.74003878   3.8890873 ]\n",
      " [-14.89343658  -2.65165043 -13.70019389   3.62392225]]\n",
      "[[4.79433566e-05 3.22098620e-05 1.71943035e-03 9.98200416e-01]\n",
      " [5.97319598e-01 1.28333499e-03 4.01298182e-01 9.88847812e-05]\n",
      " [1.46423661e-02 4.87223528e-04 2.37021787e-01 7.47848624e-01]\n",
      " [9.06143069e-09 1.87817885e-03 2.98824011e-08 9.98121782e-01]]\n",
      "[[-0.75220098 -1.50668721]\n",
      " [-2.29564869 -6.58686077]\n",
      " [-1.07141415 -2.47595506]\n",
      " [-0.74812187 -1.4971829 ]]\n",
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "W_Q = np.array([[1, -1.5], [0, 2], [-0.5, -1]])\n",
    "W_K = np.array([[-1.5, -1], [2.5, 0], [0.5, -1]])\n",
    "W_V = np.array([[1, 2.5], [-0.5, -2], [0, -1]])\n",
    "\n",
    "#print(W_Q)\n",
    "#print(W_K)\n",
    "#print(W_V)\n",
    "\n",
    "Q = X.dot(W_Q)\n",
    "K = X.dot(W_K)\n",
    "V = X.dot(W_V)\n",
    "\n",
    "print(Q)\n",
    "print(K)\n",
    "print(V)\n",
    "\n",
    "print(np.sqrt(np.size(Q, 1)))\n",
    "\n",
    "scores = Q.dot(K.T) / np.sqrt(np.size(Q, 1))\n",
    "probabilities = np.exp(scores) / np.sum(np.exp(scores), axis=1)[:, None]\n",
    "\n",
    "Z = probabilities.dot(V)\n",
    "\n",
    "print(scores)\n",
    "print(probabilities)\n",
    "print(Z)\n",
    "\n",
    "print(probabilities.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8c2c92f1f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.79433566e-05 3.22098620e-05 1.71943035e-03 9.98200416e-01]\n",
      " [5.97319598e-01 1.28333499e-03 4.01298182e-01 9.88847812e-05]\n",
      " [1.46423661e-02 4.87223528e-04 2.37021787e-01 7.47848624e-01]\n",
      " [9.06143069e-09 1.87817885e-03 2.98824011e-08 9.98121782e-01]]\n",
      "[[-0.75220098 -1.50668721]\n",
      " [-2.29564869 -6.58686077]\n",
      " [-1.07141415 -2.47595506]\n",
      " [-0.74812187 -1.4971829 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.18306921e-01 2.01966211e-02 1.68483136e-01 6.93013322e-01]\n",
      " [8.48429312e-04 9.98944583e-01 2.06267364e-04 7.20592823e-07]\n",
      " [2.67590116e-02 7.79843042e-04 5.42703523e-02 9.18190793e-01]\n",
      " [2.47463407e-05 6.12522986e-10 2.06437556e-04 9.99768815e-01]]\n",
      "[[-2.26628332 -2.26628332]\n",
      " [ 1.99725652  1.99725652]\n",
      " [-2.82066255 -2.82066255]\n",
      " [-2.99952526 -2.99952526]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.75220098 -1.50668721 -2.26628332 -2.26628332]\n",
      " [-2.29564869 -6.58686077  1.99725652  1.99725652]\n",
      " [-1.07141415 -2.47595506 -2.82066255 -2.82066255]\n",
      " [-0.74812187 -1.4971829  -2.99952526 -2.99952526]]\n",
      "[[-6.04664898  3.77781072 -0.75731086]\n",
      " [ 8.28741825  0.14750295 10.57968068]\n",
      " [-7.3905735   5.09982766 -0.01158073]\n",
      " [-8.25045389  4.87428797 -1.50140321]]\n"
     ]
    }
   ],
   "source": [
    "W_O = np.array([[-1, 1.5, 2], [0, -1, -2], [1, -1.5, 0], [2, 0, 1]])\n",
    "\n",
    "W_Q_heads = [W_Q, np.ones_like(W_Q)]\n",
    "W_K_heads = [W_K, np.ones_like(W_K)]\n",
    "W_V_heads = [W_V, np.ones_like(W_V)]\n",
    "\n",
    "head_representations = []\n",
    "\n",
    "for W_Q_h, W_K_h, W_V_h in zip(W_Q_heads, W_K_heads, W_V_heads):\n",
    "    Q_h = X.dot(W_Q_h)\n",
    "    K_h = X.dot(W_K_h)\n",
    "    V_h = X.dot(W_V_h)\n",
    "    scores = Q_h.dot(K_h.T) / np.sqrt(np.size(Q_h, 1))\n",
    "    probabilities = np.exp(scores) / np.sum(np.exp(scores), axis=1)[:, None]\n",
    "    Z_h = probabilities.dot(V_h)\n",
    "    head_representations.append(Z_h)\n",
    "    print(probabilities)\n",
    "    print(Z_h)\n",
    "    plt.imshow(probabilities)\n",
    "    plt.show()\n",
    "    \n",
    "print(np.concatenate(head_representations, axis=1))\n",
    "\n",
    "Z = np.concatenate(head_representations, axis=1).dot(W_O)\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a sequence-to-sequence network that reverses strings with the help of attention. We will randomly generate strings consisting of characters in \\{\"a\", \"b\", \"c\", \"d\"\\}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ccadbabacdbd', 'ababdcbdbab', 'dbbaabbbbccb', 'bbbdcacdbbc', 'ccac', 'cccd', 'cbddbacacdadcd', 'adabbabdcccd', 'ccad', 'bccb']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# instantiate vocabulary\n",
    "# set vocab for <start of string> and <end of string>\n",
    "# BOS = \"<s>\"\n",
    "# EOS = \"</s>\"\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, vocab):\n",
    "        self.bos = \"<s>\"\n",
    "        self.eos = \"</s>\" \n",
    "        ext_vocab = [self.bos, self.eos] + raw_vocab\n",
    "        self.index2string = {i: n for i, n in enumerate(ext_vocab)}\n",
    "        self.string2index = {n: i for i, n in enumerate(ext_vocab)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index2string)\n",
    "\n",
    "raw_vocab = list(\"abcd\")\n",
    "vocab = Vocab(raw_vocab)\n",
    "\n",
    "\n",
    "def sample_string(min_length, max_length):\n",
    "    length = random.randrange(min_length, max_length)\n",
    "    return \"\".join([random.choice(raw_vocab) for _ in range(length)])\n",
    "\n",
    "def sample_strings(min_length, max_length, size):\n",
    "    return [sample_string(min_length, max_length) for _ in range(size)]\n",
    "\n",
    "def to_tensor(name):\n",
    "    indices = [vocab.string2index[vocab.bos]] + [vocab.string2index[n] for n in name] + [vocab.string2index[vocab.eos]]\n",
    "    return torch.tensor(indices, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "def make_dataset(lines):\n",
    "    dataset = [(to_tensor(line), to_tensor(reversed(line))) for line in lines]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "#generate train/val splits\n",
    "train_size = 200\n",
    "valid_size = 100\n",
    "\n",
    "train_lines = sample_strings(3, 15, train_size)\n",
    "valid_lines = sample_strings(3, 15, valid_size)\n",
    "\n",
    "train_dataset = make_dataset(train_lines)\n",
    "valid_dataset = make_dataset(valid_lines)\n",
    "\n",
    "print(train_lines[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the model is an RNN-based encoder: we will use an LSTM ([nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)) module for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, bidirectional=False):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # set embedding layer \n",
    "        # compute hidden size for RNN\n",
    "        # initialise RNN module -- instantiate as an LSTM \n",
    "        # use nn.LSTM\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        if bidirectional:\n",
    "            hidden_size //= 2\n",
    "        self.rnn = nn.LSTM(\n",
    "            embedding_size, \n",
    "            hidden_size, \n",
    "            bidirectional=bidirectional, \n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        \"\"\"\n",
    "        input (LongTensor): batch x src length\n",
    "        src length (batch-length list): If given, the input will be packed\n",
    "        hidden: hidden or hidden/cell state input dimensions for the RNN type\n",
    "        \n",
    "        ==> you may need to reshape the hidden layer: bidirectional LSTM will \n",
    "            return a concatenation of the final forward and reverse hidden states \n",
    "            forward and backward are directions 0 and 1 respectively\n",
    "            check the _reshape_hidden function\n",
    "        \n",
    "        returns:\n",
    "            output (FloatTensor): batch x src length x hidden size\n",
    "            hidden_n (FloatTensor): hidden or hidden/cell state input\n",
    "                dimensions for the RNN type\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(input)\n",
    "        output, hidden_n = self.rnn(emb, hidden)\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden_n = self._reshape_hidden(hidden_n)\n",
    "        return output, hidden_n\n",
    "\n",
    "    def _merge_tensor(self, state_tensor):\n",
    "        forward_states = state_tensor[::2]\n",
    "        backward_states = state_tensor[1::2]\n",
    "        return torch.cat([forward_states, backward_states], 2)\n",
    "\n",
    "    def _reshape_hidden(self, hidden):\n",
    "        \"\"\"\n",
    "        hidden:\n",
    "            num_layers * num_directions x batch x self.hidden_size // 2\n",
    "            or a tuple of these\n",
    "        returns:\n",
    "            num_layers\n",
    "        \"\"\"\n",
    "        assert self.rnn.bidirectional\n",
    "        if isinstance(hidden, tuple):\n",
    "            return tuple(self._merge_tensor(h) for h in hidden)\n",
    "        else:\n",
    "            return self._merge_tensor(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define a decoder. We will also use an LSTM module for the decoder. We can add an attention mechanism on the outputs. This implementation works both with and without an attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, attn=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
    "        self.attn = attn\n",
    "\n",
    "    def forward(self, input, context, hidden):\n",
    "        \"\"\"\n",
    "        input (LongTensor): batch x tgt length\n",
    "        context (FloatTensor): batch x src length x hidden size\n",
    "        hidden: hidden or hidden/cell state input dimensions for the RNN type\n",
    "        returns (FloatTensor): (batch*tgt length) x output size\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(input)\n",
    "        output, hidden_n = self.rnn(emb, hidden)\n",
    "        \n",
    "        # apply attention between source context and query from\n",
    "        # decoder RNN\n",
    "        alignment = None\n",
    "        if self.attn is not None:\n",
    "            output, alignment = self.attn(output, context)\n",
    "\n",
    "        flat_output = output.contiguous().view(-1, self.rnn.hidden_size)\n",
    "        output = self.output_layer(flat_output)\n",
    "        return output, alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put them together into an encoder-decoder model class, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        src, tgt (LongTensor): (batch size x sequence length)\n",
    "        returns (FloatTensor): (batch*tgt length) x output size\n",
    "        \"\"\"\n",
    "        context, enc_hidden = self.encoder(src)\n",
    "        return self.decoder(tgt, context=context, hidden=enc_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our base model defined, we can write training and validation code. We need to consider how will examples be passed during training (with access to the target outputs) and during inference (validate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_iter, loss, optimizer):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    random.shuffle(train_iter)  # present examples in random order\n",
    "    for src, tgt in train_iter:\n",
    "        # Create the target input and output sequences\n",
    "        # Hint: the input and output are the same, but offset by one timestep\n",
    "        tgt_in = tgt[:, :-1]\n",
    "        gold = tgt[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Run the forward pass of the model\n",
    "        pred, _ = model(src, tgt_in)\n",
    "        # Compute the loss\n",
    "        batch_loss = loss(pred, gold)\n",
    "        # Backprop the loss and update the parameters\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update the epoch loss\n",
    "        epoch_loss += batch_loss.item()\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validate(model, data_iter):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_iter:\n",
    "            tgt_in = tgt[:, :-1]\n",
    "            pred = model(src, tgt_in)[0].argmax(dim=1)\n",
    "            gold = tgt[:, 1:].contiguous().view(-1)\n",
    "            n_correct += (pred == gold).sum().item()\n",
    "            n_total += gold.size(0)\n",
    "        return n_correct / n_total\n",
    "\n",
    "\n",
    "def train(model, train, valid, epochs=30, learning_rate=0.5):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_accs = []\n",
    "    epochs = list(range(1, epochs + 1))\n",
    "    for epoch in epochs:\n",
    "        #print('Training epoch {}'.format(epoch))\n",
    "        train_loss = train_epoch(model, train, loss, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_acc = validate(model, valid)\n",
    "        valid_accs.append(valid_acc)\n",
    "        #print('Train loss: {} ; Validation acc: {}'.format(train_loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a unidirectional model without attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embeddings): Embedding(6, 6)\n",
      "    (rnn): LSTM(6, 64, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embeddings): Embedding(6, 6)\n",
      "    (rnn): LSTM(6, 64, batch_first=True)\n",
      "    (output_layer): Linear(in_features=64, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/margaridacampos/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_size = vocab_size\n",
    "hidden_size = 64\n",
    "\n",
    "enc = Encoder(vocab_size, embedding_size, hidden_size)\n",
    "dec = Decoder(vocab_size, embedding_size, hidden_size)\n",
    "enc.embeddings.weight.data = torch.eye(vocab_size)\n",
    "dec.embeddings.weight.data = enc.embeddings.weight.data\n",
    "enc.embeddings.weight.requires_grad = False\n",
    "dec.embeddings.weight.requires_grad = False\n",
    "\n",
    "model = Seq2Seq(enc, dec)\n",
    "print(model)\n",
    "\n",
    "train(model, train_dataset, valid_dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model often manages to predict the right sequence, but it also often fails. Note that the decoder makes use of the *last* hidden state from the encoder, which has recently seen the final time step of the source sequence (in other words, the first element it needs to predict), but the other elements less recently. If only there were a way to make it easier for the model to focus on less recent positions...\n",
    "\n",
    "The attention mechanism is an extra layer for the decoder that can do precisely this. In this exercise, we consider a simple but effective attention mechanism called *dot product attention*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProdAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, query, context):\n",
    "        \"\"\"\n",
    "        query: batch x tgt_length x hidden_size\n",
    "        context: batch x src_length x hidden_size\n",
    "        \"\"\"\n",
    "        tgt_batch, tgt_len, tgt_hidden = query.size()\n",
    "        src_batch, src_len, src_hidden = context.size()\n",
    "        attn_scores = torch.bmm(query, context.transpose(1, 2))\n",
    "        alignment = torch.softmax(attn_scores, 2)\n",
    "        c = torch.bmm(alignment, context)\n",
    "        attn_h_t = self.mlp(torch.cat([c, query], dim=2))\n",
    "        return attn_h_t, alignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that attention has been implemented, we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embeddings): Embedding(6, 6)\n",
      "    (rnn): LSTM(6, 32, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embeddings): Embedding(6, 6)\n",
      "    (rnn): LSTM(6, 64, batch_first=True)\n",
      "    (output_layer): Linear(in_features=64, out_features=6, bias=True)\n",
      "    (attn): DotProdAttention(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "attn = DotProdAttention(hidden_size)\n",
    "enc = Encoder(vocab_size, embedding_size, hidden_size, bidirectional=True)\n",
    "dec = Decoder(vocab_size, embedding_size, hidden_size, attn=attn)\n",
    "enc.embeddings.weight.data = torch.eye(vocab_size)\n",
    "dec.embeddings.weight.data = enc.embeddings.weight.data\n",
    "enc.embeddings.weight.requires_grad = False\n",
    "dec.embeddings.weight.requires_grad = False\n",
    "\n",
    "attn_model = Seq2Seq(enc, dec)\n",
    "print(attn_model)\n",
    "\n",
    "train(attn_model, train_dataset, valid_dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the model's attention matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8c1fb0c490>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "string = \"abacadabacc\"  # try something\n",
    "reversed_string = reversed(string)\n",
    "\n",
    "src = to_tensor(string)\n",
    "tgt = to_tensor(reversed_string)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, alignment = attn_model(src, tgt)\n",
    "\n",
    "attn_matrix = alignment.squeeze(0).numpy()\n",
    "plt.matshow(attn_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
